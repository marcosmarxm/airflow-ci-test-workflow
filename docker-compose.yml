# Airflow está evoluindo rapidamente e por isso talvez o docker e comandos
# utilizados aqui podem falhar num futuro proximo. O problema que enfrento
# agora é ter que esperar alguns minutos até conseguir enviar comandos
# para o Airflow. 
# configurar um pipeline automatico para rodar semanalmente.

version: '3.6'

services:

  airflow:
      image: ${REPO}
      container_name: airflow
      volumes:
        - ./dags:/opt/airflow/dags
        - ./sql:/opt/airflow/sql
        - ./tests:/opt/airflow/tests
        - ./data:/opt/airflow/data
        - ./requirements.txt:/opt/airflow/requirements.txt
      environment: 
        - AIRFLOW_HOME=/opt/airflow
        - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
        - AIRFLOW__CORE__EXECUTOR=${EXECUTOR}
        - AIRFLOW__CORE__LOAD_EXAMPLES=false
        - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=false
        - AIRFLOW__API__AUTH_BACKEND=${AIRFLOW_API_BACKEND}
      ports:
        - 8080:8080
      command: 'bash -c "pip3 install -r requirements.txt && airflow db init && airflow webserver -D && airflow scheduler -D"'

  oltp-db:
    image: postgres:11.4-alpine
    restart: always
    container_name: oltp-db
    ports:
      - 54320:54320
    environment:
      - POSTGRES_USER=${SOURCE_USER}
      - POSTGRES_PASSWORD=${SOURCE_PASSWORD}
      - POSTGRES_DB=${SOURCE_DB}

  olap-db:
    image: postgres:11.4-alpine
    restart: always
    container_name: olap-db
    ports:
      - 54321:54321
    environment:
      - POSTGRES_USER=${DEST_USER}
      - POSTGRES_PASSWORD=${DEST_PASSWORD}
      - POSTGRES_DB=${DEST_DB}
